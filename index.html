<!DOCTYPE html><html lang="en"><head>
	<meta name="generator" content="Hugo 0.152.2">
    <meta charset="utf-8">
<title>Structure and Automate AI Workflows with MLOps</title>
<meta name="description" content="Introduction to ML- and LLM-Ops">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><link rel="stylesheet" href="/talk-2025-mlops/reveal-js/dist/reset.css">
<link rel="stylesheet" href="/talk-2025-mlops/reveal-js/dist/reveal.css">
  <link rel="stylesheet" href="/talk-2025-mlops/css/custom-theme.min.f96d10ca3b5f41b4315bb018de34bcf5b99347c92a09993533ce4f444ed31c56.css" id="theme"><link rel="stylesheet" href="/talk-2025-mlops/highlight-js/solarized-dark.min.css">
<link href="https://fonts.googleapis.com/css?family=Roboto Mono" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Oxygen Mono" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu Mono" rel="stylesheet">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
<link href="https://cdn.jsdelivr.net/gh/DanySK/css-blur-animation/blur.css" rel="stylesheet" crossorigin="anonymous">

<script src="https://kit.fontawesome.com/81ac037be0.js" crossorigin="anonymous"></script>
<script type="text/javascript" src="https://unpkg.com/qr-code-styling@1.5.0/lib/qr-code-styling.js"></script>

  </head>
  <body>
    
    <div class="reveal">
      <div class="slides">
  

    <section>

<section data-shortcode-section="">
<h1 id="structure-and-automate-ai-workflows-with-mlops">Structure and Automate AI Workflows with MLOps</h1>
<p><a href="mailto:giovanni.ciatto@unibo.it">Giovanni Ciatto</a>
<br> Dipartimento di Informatica — Scienza e Ingegneria (DISI), Sede di Cesena,
<br> Alma Mater Studiorum—Università di Bologna</p>
<!-- 




<img src='./front.png' alt='' style='max-width: 95vw; max-height: 50vh; object-fit: contain;'>
 -->
<p><span class="hint">(versione presentazione: 2025-10-28
)</span></p>
</section><section>
<h2 id="link-a-queste-slide">Link a queste slide</h2>
<p><a href="https://gciatto.github.io/talk-2025-mlops/">https://gciatto.github.io/talk-2025-mlops/</a></p>
<div id="ZTI2ZDA3MTYwNWExNjNhN2I0NWQ5MGQyMDJlMjQxYjkyYzI3MTUwYTk5YzRkZTY4MWYyOWIyOTEyMTAwMGZjMg==" style=""></div>
<script type="text/javascript">
    const qrCode = new QRCodeStyling({
        width:  300 ,
        height:  300 ,
        type: "svg",
        data: "%!s(\u003cnil\u003e)",
        image: "https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg",
        dotsOptions: {
            color: "#000000",
            type: "rounded"
        },
        backgroundOptions: {
            color: "#ffffff",
        },
        imageOptions: {
            crossOrigin: "anonymous",
            margin:  10 
        }
    });
    qrCode.append(document.getElementById("ZTI2ZDA3MTYwNWExNjNhN2I0NWQ5MGQyMDJlMjQxYjkyYzI3MTUwYTk5YzRkZTY4MWYyOWIyOTEyMTAwMGZjMg=="));
</script>
<p><a href="?print-pdf&amp;pdfSeparateFragments=false"><i class="fa fa-print" aria-hidden="true"></i> versione stampabile</a></p>
</section>
<section data-noprocess="" data-shortcode-slide="" id="toc">
<h2 id="outline">Outline</h2>
<ol>
<li>
<p>Motivation and Context</p>
<ul>
<li>the ML workflow</li>
<li>the GenAI workflow</li>
<li>need for MLOps, definition, expected benefits</li>
</ul>
</li>
<li>
<p>MLOps with MLflow</p>
<ul>
<li>API, tracking server, backend store, artifact store, setups</li>
<li>interactive usage (notebook)</li>
<li>batch usage + project setup</li>
<li>interoperability with Python libraries</li>
</ul>
</li>
<li>
<p>End-to-end example for classification</p>
</li>
<li>
<p>End-to-end example for LLM agents</p>
</li>
</ol>

</section>
</section><section>
<h2 id="what-is-the-goal-of-a-machine-learning-workflow">What is the <em>goal</em> of a Machine Learning workflow?</h2>
<p>Training a <strong>model</strong> from <em>data</em>, in order to:</p>
<ul>
<li>do <strong>prediction</strong> on <em>unseen data</em>,
<ul>
<li>e.g. spam filter</li>
</ul>
</li>
<li>or <strong>mine</strong> information from it,
<ul>
<li>e.g. profiling customers</li>
</ul>
</li>
<li>or <strong>automate</strong> some operation which is <em>hard to code</em> explicitly
<ul>
<li>e.g. NPCs in video games</li>
</ul>
</li>
</ul>
</section><section>


<section data-shortcode-section="">
<h2 id="what-is-a-model-in-the-context-of-ml-pt-1">What is a <em>model</em> in the context of ML? (pt. 1)</h2>
<p>In <em>statistics</em> (and <em>machine learning</em>) a <strong>model</strong> is a <em>mathematical representation</em> of a real-world process
<br> (commonly attained by <em>fitting</em> a parametric <em>function</em> over a <em>sample</em> of <em>data</em> describing the process)</p>
<p><img src="./model-statistics.webp" alt=""></p>
<p>e.g.: <strong>$f(x) = \beta_0 + \beta_1 x $</strong> where <strong>$f$</strong> is the amount of minutes played, and <strong>$x$</strong> is the age</p>
</section><section>
<h2 id="what-is-a-model-in-the-context-of-ml-pt-2">What is a <em>model</em> in the context of ML? (pt. 2)</h2>
<p>E.g. <strong>neural networks</strong> (NN) are a popular <em>family</em> of models</p>
<div class="container w-100 m-0 p-0">
    <div class="row "><div class="col "><img src="./neuron.png" alt="Functioning of a single neuron" style="width: 100%; max-width: 95vw; max-height: 60vh; object-fit: contain;">
<p>Single neuron</p>
</div>
<div class="col "><img src="./neural-network.png" alt="Functioning of a feed-forward neural network" style="width: 100%; max-width: 95vw; max-height: 60vh; object-fit: contain;">
<p>(Feed-forward)
<br>
Neural network $\equiv$ cascade of <em>layers</em></p>
</div>
<div class="col "><img src="./nn-zoo.png" alt="Many sorts of neural architectures" style="width: 100%; max-width: 95vw; max-height: 60vh; object-fit: contain;">
<p><a href="https://www.asimovinstitute.org/neural-network-zoo/">Many admissible architectures</a>, serving disparate purposes</p>
</div>
</div>
</div>

</section>
</section><section>
<h2 id="what-is-the-outcome-of-a-machine-learning-workflow">What is the <em>outcome</em> of a Machine Learning workflow?</h2>
<ul>
<li>
<p>A <strong>software module</strong> (e.g. a Python object) implementing a <em>mathematical function</em>…</p>
<ul>
<li>e.g. <code>predict(input_data) -&gt; output_data</code></li>
</ul>
</li>
<li>
<p>… commonly <strong>tailored</strong> on a specific <em>data schema</em></p>
<ul>
<li>e.g. customer information + statistics about shopping history</li>
</ul>
</li>
<li>
<p>… which <strong>works</strong> sufficiently <strong>well</strong> w.r.t. <em>test data</em></p>
</li>
<li>
<p>… which must commonly be <strong>integrated</strong> into a much larger <em>software system</em></p>
<ul>
<li>e.g. a web application, a mobile app, etc.</li>
</ul>
</li>
<li>
<p>… which may need to be <strong>re-trained</strong> upon <em>data changes</em>.</p>
</li>
</ul>
</section><section>


<section data-shortcode-section="">
<h2 id="what-are-the-phases-of-a-machine-learning-workflow">What are the <em>phases</em> of a Machine Learning workflow?</h2>
<p>The process of producing a ML model is <strong>not</strong> <em>linear</em> <strong>nor</strong> <em>simple</em>:</p>
<p><img src="./ml-workflow.webp" alt=""></p>
<ul>
<li>there could be <strong>many iterations</strong> (up to reaching <em>satisfactory evaluation</em>)</li>
<li>the whole workflow may be <strong>re-started</strong> upon <em>data changes</em></li>
<li>updates in the model imply further <strong>integration</strong>/deployment <em>efforts</em> in <em>downstream systems</em></li>
</ul>
</section><section>
<h2 id="activities-in-a-typical-ml-workflow">Activities in a typical ML workflow</h2>
<ol>
<li><strong>Problem framing</strong>: define the business/technical goal</li>
<li><strong>Data collection</strong>: acquire raw data</li>
<li><strong>Data preparation</strong>: clean, label, and transform data</li>
<li><strong>Feature engineering</strong>: extract useful variables from data</li>
<li><strong>Model training</strong>: apply ML algorithms to produce candidate models</li>
<li><strong>Experimentation &amp; evaluation</strong>: compare models, tune hyperparameters, measure performance</li>
<li><strong>Model packaging &amp; deployment</strong>: turn the best model into a service or product</li>
<li><strong>Monitoring &amp; feedback</strong>: check performance in production, detect drift, gather new data, trigger retraining</li>
</ol>
<blockquote>
<p>These steps are cyclical, not linear → one often revisits data, retrain, or refine features.</p>
</blockquote>
</section><section>
<h2 id="example-of-ml-workflow">Example of ML workflow</h2>
<blockquote>
<p>Forecast footfall/visits to some office by day/time</p>
</blockquote>
<ul>
<li>useful for staffing and opening hours planning</li>
</ul>
<ol>
<li><strong>Problem framing</strong>: model as a <em>regression</em> task or <em>time-series forecasting</em> task?</li>
<li><strong>Data collection</strong>: gather <em>historical</em> footfall <em>data</em>, calendar events, weather data, etc.</li>
<li><strong>Data preparation</strong>: clean and preprocess data, handle missing values, etc.</li>
<li><strong>Feature engineering</strong>: create <em>relevant features</em> (e.g. day of week, holidays, weather conditions)</li>
<li><strong>Model training</strong>: apply ML algorithms to <em>produce candidate models</em></li>
<li><strong>Experimentation &amp; evaluation</strong>: <em>compare models</em>, tune hyperparameters, measure performance</li>
<li><strong>Model packaging &amp; deployment</strong>: turn the <em>best model</em> into a <em>service</em> or product</li>
<li><strong>Monitoring &amp; feedback</strong>: <em>monitor performance</em> in production, detect <em>drifts</em>, gather new data, trigger <em>retraining</em>
<ul>
<li>new offices or online services may change footfall patterns</li>
</ul>
</li>
</ol>

</section>
</section><section>
<h2 id="how-are-machine-learning-workflows-typically-performed">How are Machine Learning workflows typically performed?</h2>

<div class="container w-100 m-0 p-0">
    <div class="row "><div class="col "><p><img src="example-jupyter-notebook.png" alt=""></p>
</div>
<div class="col "><h3 id="via-notebooks-eg-jupyter">Via Notebooks (e.g. Jupyter)</h3>
<ul>
<li>
<p>✅ Interleave code, textual description, and visualizations</p>
</li>
<li>
<p>✅ Interactive usage, allowing for real-time feedback and adjustments</p>
</li>
<li>
<p>✅ Uniform &amp; easy interface to workstations</p>
</li>
<li>
<p>✅ Easy to save, restore, and share</p>
</li>
<li>
<p>❌ Incentivises manual activities over automatic ones</p>
</li>
</ul>
</div>
</div>
</div>
</section><section>
<h2 id="pitfalls-of-manual-work-in-notebooks">Pitfalls of manual work in notebooks</h2>
<ul>
<li><strong>Non-reproducibility</strong>: hidden state, out-of-order execution, forgotten seeds</li>
<li><strong>Weak provenance</strong>: params, code version, data slice, and metrics not logged</li>
<li><strong>Human-in-the-loop gating</strong>: “print accuracy → eyeball → tweak → rerun”</li>
<li><strong>Fragile artifacts</strong>: models overwritten, files named <code>final_v3.ipynb</code></li>
<li><strong>Environment drift</strong>: “works on my machine” dependencies and data paths</li>
<li><strong>Collaboration pain</strong>: merge conflicts, opaque diffs, reviewability issues</li>
</ul>
</section><section>
<h2 id="example-why-manual-runs-mislead">Example: why manual runs mislead</h2>
<ul>
<li>Run 1: random split → train → print accuracy = 0.82</li>
<li>Tweak hyperparams → rerun only training cell → accuracy = 0.86</li>
<li>Forgot to fix seed / re-run split → different data, different metric</li>
<li>No record of params, code, data; “best” model cannot be justified</li>
</ul>


<span class="fragment ">
  <h3 id="consequences">Consequences</h3>
<ul>
<li>Incomparable results, irreproducible models</li>
<li>Hard to automate, schedule, or roll back</li>
<li>No trace from model → code → data → metrics</li>
</ul>

</span>

</section><section>
<h2 id="comparison-among-ml-and-ordinary-software-projects">Comparison among ML and ordinary software projects</h2>
<h3 id="analogies">Analogies</h3>
<ul>
<li>Both <strong>produce</strong> <em>software modules</em> in the end</li>
<li>Both involve <strong>iterative processes</strong>, where <em>feedback</em> is used to improve the product</li>
<li>Both are driven by <strong>tests</strong>/evaluations</li>
<li>Both may benefit from <strong>automation</strong>
<ul>
<li>… and may <em>lose efficiency</em> when activities are performed manually</li>
</ul>
</li>
</ul>


<span class="fragment ">
  <h3 id="differences">Differences</h3>
<ul>
<li>ML projects depend on <em>data</em> (which <em>changes</em> over time)</li>
<li>Models need <em>training</em> and <em>retraining</em>, not just coding</li>
<li>Performance may <em>degrade</em> in production (data drift, bias, new environments)</li>
<li>Many <em>different expertises</em> are involved (data engineers, software engineers, domain experts, operations)</li>
</ul>

</span>



<span class="fragment ">
  <blockquote>
<p>No structured process $\implies$ ML projects may fail to move from notebooks to real-world use</p>
</blockquote>

</span>

</section><section>
<h2 id="machine-learning-operations-mlops">Machine Learning Operations (<a href="https://en.wikipedia.org/wiki/MLOps">MLOps</a>)</h2>
<blockquote>
<p>The practice of organizing and <strong>automating</strong> the <em>end-to-end</em> process of building, training, deploying, and maintaining <em>machine-learning models</em></p>
</blockquote>


<span class="fragment ">
  <h3 id="expected-benefits">Expected benefits</h3>
<ul>
<li><strong>Reproducibility</strong> → the same code + same data always gives the same model</li>
<li><strong>Automation</strong> → repetitive steps (training, testing, deployment) are handled by pipelines</li>
<li><strong>Scalability</strong> → easier to scale up the training process to more data, bigger models, or more computing resources</li>
<li><strong>Monitoring &amp; governance</strong> → models are tracked, evaluated, and kept under control</li>
<li><strong>Collaboration</strong> → teams work on shared infrastructure, with clear responsibilities</li>
<li><strong>Versioning</strong> → models, data, and code are versioned and traceable</li>
</ul>

</span>

</section><section>
<h2 id="how-does-mlops-support-ml-practitioners">How does MLOps support ML practitioners</h2>
<p>MLOps adds <em>infrastructure</em> + <em>processes</em> + <em>automation</em> to make each step more reliable:</p>
<ul>
<li><strong>Data</strong> → <em>version control</em> for datasets, metadata, lineage tracking</li>
<li><strong>Training</strong> → <em>automated pipelines</em> that reproduce experiments on demand</li>
<li><strong>Evaluation</strong> → <em>systematic tracking</em> of metrics, logs, and artifacts</li>
<li><strong>Deployment</strong> → continuous integration &amp; delivery (<em>CI/CD</em>) for ML models, often with <em>model registries</em></li>
<li><strong>Monitoring</strong> → <em>automated checks</em> for performance, drift, fairness, anomalies</li>
<li><strong>Collaboration</strong> → <em>shared repositories</em>, environments, and documentation so teams can work together</li>
</ul>
</section><section>
<h2 id="what-may-happen-without-mlops">What may happen <strong>without</strong> MLOps</h2>
<ul>
<li><strong>Data</strong> in <em>ad-hoc spreadsheets</em> or <em>local files</em> (no version control)</li>
<li><strong>Training</strong> in <em>personal notebooks</em> (hard to reproduce later)</li>
<li><strong>Model evaluation</strong> is <em>manual</em> and <em>undocumented</em> (hard to compare results)</li>
<li><strong>Deployment</strong> = <em>copy-paste</em> code or manual sharing of a <em>model file</em></li>
<li><strong>Monitoring</strong> is much harder → <em>models silently degrade</em></li>
<li><strong>Collaboration</strong> = <code>“send me your notebook by email”</code></li>
</ul>


<span class="fragment ">
  <h3 id="consequences">Consequences</h3>
<ul>
<li>❌ Fragile, non-reproducible workflows</li>
<li>❌ Long delays when models need updating</li>
<li>❌ Difficulty scaling beyond a single researcher</li>
<li>❌ Low trust from stakeholders (“why did accuracy drop?”)</li>
</ul>

</span>

</section><section>
<h1 id="what-about-generative-ai-workflows">What about Generative AI workflows?</h1>
</section><section>
<h2 id="what-is-the-goal-of-a-generative-ai-workflow">What is the <em>goal</em> of a Generative AI workflow?</h2>
<p>Engineering <em>prompts</em>, <em>tools</em>, <em>vector stores</em>, and <em>agents</em> to constrain and govern the behavior of <strong>pre-trained</strong> (<em>foundation</em>) models, in order to:</p>
<ul>
<li><strong>generate</strong> contents (text, images, code, etc.) for a specific purpose
<ul>
<li>e.g. bring unstructured data into a particular format</li>
<li>e.g. produce summaries, reports, highlights</li>
</ul>
</li>
<li><strong>interpret</strong> unstructured data and <em>grasp information</em> from it
<ul>
<li>e.g. extract entities, relations, sentiments</li>
<li>e.g. answer questions about a document</li>
</ul>
</li>
<li><strong>automate</strong> data-processing tasks which are <em>hard to code</em> explicitly
<ul>
<li>e.g. the task is ill-defined (<code>write an evaluation paragraph for each student's work</code>)</li>
<li>e.g. the task requires mining information from unstructured data (<code>find the parties involved in this contract</code>)</li>
<li>e.g. the task is complex yet too narrow to allow for general purpose coding (<code>plan a vacation itinerary based on user preferences</code>)</li>
</ul>
</li>
<li><strong>interact</strong> with users via <em>natural language</em>
<ul>
<li>e.g. chatbots, virtual assistants</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="lets-explain-the-nomenclature">Let’s explain the nomenclature</h2>
<ul>
<li>
<p><strong>Pre-trained <u>foundation</u> models</strong> (PFM): large neural-networks trained on massive datasets to learn general skills (e.g. ‘understanding’ and generating text, images, code)</p>
<ul>
<li>e.g. GPT, PaLM, LLaMA, etc.</li>
</ul>
</li>
<li>
<p><strong>Prompts</strong>: carefully <em>crafted textual inputs</em> that guide some PFM to produce <em>desired outputs</em></p>
<ul>
<li>prompt <strong>templates</strong> are prompts with <em>named placeholders</em> to be filled with specific data at runtime
<ul>
<li>e.g. <code>Write a summary of the following article: {article_text}</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Tools</strong>: external <em>software components</em> (e.g. APIs, databases, search engines) that can be <em>invoked</em> by PFMs to perform specific tasks or retrieve information</p>
<ul>
<li>e.g. a calculator API, a weather API, a database query interface</li>
</ul>
</li>
<li>
<p><strong>Vector stores</strong>: specialized databases that store and retrieve <em>high-dimensional vectors</em> (embeddings) for the sake of <em>information retrieval</em> via <em>similarity search</em></p>
<ul>
<li>e.g. to support <em>retrieval-augmented generation</em> (RAG)</li>
</ul>
</li>
<li>
<p><strong>Agents</strong>: software systems that <em>orchestrate</em> the interaction between PFMs and tools, enabling dynamic decision-making and task execution based on the context and user input</p>
<ul>
<li>e.g. a chatbot that uses a PFM for conversation and invokes a weather API when asked about the weather</li>
<li>e.g. an assistant that uses a PFM to understand user requests and a database to fetch relevant information</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="what-are-the-outcomes-of-a-generative-ai-workflow">What are the <em>outcomes</em> of a Generative AI workflow?</h2>
<ol start="0">
<li>
<p>FM are commonly <u>not</u> produced in-house, but rather <em>accessed</em> via APIs… yet the choice of <strong>what model(s) to use</strong> is crucial</p>
<ul>
<li>must be available, configured, and most commonly imply <em>costs</em> (per call, per token, etc.)</li>
</ul>
</li>
<li>
<p>A set of <strong>prompt templates</strong> (text files, or code snippets) that are known to work well for the tasks at hand</p>
<ul>
<li>commonly assessed via semi-automatic <em>evaluations</em> on a <em>validation set</em> of inputs</li>
</ul>
</li>
<li>
<p>A set of <strong>tool servers</strong> implementing the <a href="https://modelcontextprotocol.io/docs/getting-started/intro">MCP protocol</a> so that tools can be <em>invoked</em> by PFMs</p>
<ul>
<li>these are <em>software modules</em>, somewhat similar to ordinary Web services, offering one endpoint per tool</li>
</ul>
</li>
<li>
<p>A set of <strong>agents</strong>, implementing the logic to orchestrate the interaction between PFMs and tools</p>
<ul>
<li>these are <em>software modules</em>, commonly implemented via libraries such as <a href="https://python.langchain.com/en/latest/index.html">LangChain</a> or <a href="https://gpt-index.readthedocs.io/en/latest/">LlamaIndex</a></li>
</ul>
</li>
<li>
<p>A set of <strong>vector stores</strong> (if needed), populated with relevant data, and accessible by the agents</p>
<ul>
<li>there are <em>software modules</em>, somewhat similar to ordinary DBMS, offering CRUD operations on data chunks <em>indexed by</em> their <em>embeddings</em></li>
</ul>
</li>
</ol>
</section><section>


<section data-shortcode-section="">
<h2 id="what-are-the-phases-of-a-genai-workflow">What are the <em>phases</em> of a GenAI workflow?</h2>
<p>(Similar to the ML workflow in the sense that the goal is to process data, but different in many details e.g. <em>no training</em> is involved)</p>
<p><img src="./genai-workflow.png" alt=""></p>
<ul>
<li>there could be <strong>many iterations</strong> (e.g. for PFM selection, and prompt tuning)</li>
<li>the whole workflow may be <strong>re-started</strong> upon <em>data changes</em>, or <em>task changes</em>, or new <em>PFM availability</em></li>
<li>the <strong>interplay</strong> between prompts, models, tasks, and data may need to be <em>monitored</em> and <em>adjusted</em> continuously</li>
<li>the <strong>data-flow</strong> between components (agents, PFM, tools, vector stores) may need to be <em>tracked</em> for the sake of <em>debugging</em> and <em>monitoring</em></li>
</ul>
</section><section>
<h2 id="peculiar-activities-in-a-typical-genai-workflow">Peculiar activities in a typical GenAI workflow</h2>
<ol>
<li>
<p><strong>Foundation model selection</strong>: choose the most suitable pre-trained model(s) based on task requirements, performance, cost, data protection, and availability</p>
<ul>
<li>implies trying out prompts (even manually) on different models</li>
</ul>
</li>
<li>
<p><strong>Prompt engineering</strong>: design, test, and refine prompt templates to elicit the desired responses</p>
<ul>
<li>implies engineering variables, lengths, formats, contents, etc</li>
</ul>
</li>
<li>
<p><strong>Evaluations</strong>: establish assertions and metrics to assess PFM responses to prompts (attained by instantiating templates over actual data)</p>
<ul>
<li>somewhat similar to <em>unit tests</em> in ordinary software</li>
<li>important when automatic, as they allow quick evaluations on prompt/model combinations</li>
</ul>
</li>
<li>
<p><strong>Tracking</strong> the <em>data-flow</em> between components (agents, PFM, tools, vector stores) to monitor <em>costs</em>, <em>latency</em>, and to <em>debug</em> unexpected behaviors</p>
<ul>
<li>also useful for the sake of <em>auditing</em> and <em>governance</em></li>
</ul>
</li>
</ol>
</section><section>
<h2 id="example-of-genai-workflow-pt-1">Example of GenAI workflow (pt. 1)</h2>
<blockquote>
<p>Support public officers in managing tenders through a GenAI assistant that understands and compares procurement decisions transparently.</p>
</blockquote>
<ol>
<li>
<p><strong>Problem Framing</strong>:</p>
<ul>
<li><em>Content Generation</em>: draft and justify <em>comparisons</em> among suppliers’ offers vs. technical specs</li>
<li><em>Interpretation</em>: understand regulatory documents and technical language</li>
<li><em>Automation</em>: retrieve relevant laws, norms, and prior tender examples</li>
<li><em>Interaction</em>: enable officers to query and validate results through natural language</li>
</ul>
</li>
<li>
<p><strong>Data Collection</strong>: past tenders’ technical specifications, acts, etc; regulatory documents, etc.</p>
</li>
<li>
<p><strong>Data Preparation</strong>:</p>
<ul>
<li>devise useful data schema &amp; extract relevant data from documents</li>
<li>anonymize sensitive info (suppliers, personal data)</li>
<li>segment documents and index by topic (law, SLA, price table, etc.)</li>
</ul>
</li>
</ol>
</section><section>
<h2 id="example-of-genai-workflow-pt-2">Example of GenAI workflow (pt. 2)</h2>
<ol start="4">
<li>
<p><strong>Prompt Engineering</strong>:</p>
<ol>
<li>design prompt templates for comparison, justification, and Q&amp;A
<ul>
<li>use role-based system prompts (<code>You are a procurement evaluator…</code>)</li>
</ul>
</li>
<li>allocate placeholders for RAG-retrieved data chunks</li>
<li>iterate on template design based on manual tests</li>
</ol>
</li>
<li>
<p><strong>Foundation Model Selection</strong>: multi-lingual? specialized in legal/technical text? cost constraints? support for tools?</p>
</li>
<li>
<p><strong>Vector stores</strong>: storing embeddings for tender documents &amp; specs, legal texts &amp; guidelines, previous evaluation, templates</p>
<ol>
<li>choose embedding model, chunking strategy, and populate vector store</li>
<li>engineer retrieval strategies to fetch relevant chunks</li>
</ol>
</li>
<li>
<p><strong>Tools</strong>:</p>
<ul>
<li>regulation lookup API + tender database query API</li>
<li>report generation out of document templates</li>
<li>automate scoring calculations via spreadsheet or Python scripts generation</li>
</ul>
</li>
<li>
<p><strong>Agents</strong>:</p>
<ol>
<li>exploit LLM to extract structured check-lists out of technical specs</li>
<li>orchestrate RAG, tool invocations, and prompt templates to score each offer</li>
<li>generate comparison reports</li>
<li>…</li>
</ol>
</li>
</ol>

</section>
</section><section>
<h2 id="llm-operations-llmops">LLM Operations (<a href="https://www.databricks.com/it/glossary/llmops">LLMOps</a>)</h2>
<blockquote>
<p>The practice of organizing and <strong>automating</strong> the <em>end-to-end</em> process of building, evaluating, deploying, and maintaining <em>GenAI applications</em></p>
</blockquote>
<br>


<span class="fragment ">
  <h4 id="in-a-nutshell-mlops-for-genai">In a nutshell: <strong>MLOps for GenAI</strong></h4>

</span>

<br>


<span class="fragment ">
  <h3 id="expected-benefits">Expected benefits</h3>
<ul>
<li><strong>Systematicity</strong> → structured processes to manage prompts, tools, and agents</li>
<li><strong>Efficiency</strong> → reuse of components, templates, and evaluations</li>
<li><strong>Scalability</strong> → easier to test, and update individual components (prompt templates, tools, agents)</li>
<li><strong>Monitoring &amp; governance</strong> → components are tracked, evaluated, and kept under control</li>
</ul>

</span>

</section><section>
<h2 id="how-does-llmops-support-genai-practitioners">How does LLMOps support GenAI practitioners</h2>
<p>LLOps adds <em>infrastructure</em> + <em>processes</em> + <em>automation</em> to make each step more reliable:</p>
<ul>
<li><strong>Foundation models</strong> → <em>catalogs</em> of available models, with metadata on capabilities, costs, and usage policies</li>
<li><strong>Provider Gateways</strong> → standardized APIs to access different PFM providers (e.g. OpenAI, HuggingFace) uniformly, without code rewrites</li>
<li><strong>Prompt engineering</strong> → <em>version control</em> for prompt templates, systematic testing frameworks</li>
<li><strong>Tool integration</strong> → <em>standardized protocols</em> (e.g. MCP) and libraries to connect tools with PFMs + <em>gateway technologies</em> to aggregate multiple tools</li>
<li><strong>Agents</strong> → <em>provider-agnostic libraries</em> and frameworks (e.g. LangChain) to build, manage, and orchestrate agents</li>
<li><strong>Vector stores</strong> → <em>standardized interfaces</em> to store and retrieve data chunks via embeddings, with support for <em>multiple backend</em> DBMS</li>
<li><strong>Evaluation &amp; monitoring</strong> → <em>automated</em> frameworks to run <em>evaluations</em>, <em>track performance</em>, and <em>monitor costs</em></li>
</ul>
</section><section>
<h2 id="what-may-happen-without-llmops">What may happen <strong>without</strong> LLMOps</h2>
<ul>
<li>
<p><strong>Foundation models</strong> are <em>hard-coded</em> in the application</p>
<ul>
<li>making it difficult to switch providers or models</li>
</ul>
</li>
<li>
<p><strong>Prompt templates</strong> are <em>scattered</em> in code or documents</p>
<ul>
<li>making it hard to track changes or reuse them</li>
</ul>
</li>
<li>
<p><strong>Tools</strong> are <em>manually integrated</em>, leading to:</p>
<ul>
<li>brittle connections,</li>
<li>lack of observability,</li>
<li>maintenance challenges</li>
</ul>
</li>
<li>
<p><strong>Agents</strong> are <em>ad-hoc scripts</em> that mix logic, PFM calls, and tool invocations</p>
<ul>
<li>making them hard to debug, extend or compose</li>
</ul>
</li>
<li>
<p><strong>Vector stores</strong> are <em>tightly coupled</em> with specific DBMS</p>
<ul>
<li>making it hard to migrate or scale</li>
</ul>
</li>
<li>
<p><strong>Evaluation &amp; monitoring</strong> are <em>manual</em> and <em>sporadic</em> leading to undetected issues, cost overruns, and loss of trust</p>
</li>
</ul>
</section><section>
<h1 id="mlops-and-llmops-with-mlflow">MLOps and LLMOps with <a href="https://mlflow.org/">MLflow</a></h1>
</section><section>
<h2 id="what-is-mlflow">What is MLflow? <a href="https://mlflow.org/">https://mlflow.org/</a></h2>
<p><img src="./mlflow-logo.webp" alt=""></p>
<blockquote>
<p>An <em>open-source</em> Python framework for <strong>MLOps</strong> and (most recently) <strong>LLMOps</strong></p>
</blockquote>
<ul>
<li>usable either in-cloud (e.g. via <a href="https://www.databricks.com/">Databricks</a>) or on-premises (self-hosted)
<ul>
<li>we’ll see the latter setup</li>
</ul>
</li>
</ul>


<span class="fragment ">
  <h3 id="outline">Outline</h3>
<ol>
<li>First, we focus on how to use MLflow for the sake of <em>MLOps</em></li>
<li>Then, we show how MLflow can be used for <em>LLMOps</em> as well</li>
</ol>

</span>

</section><section>
<h2 id="mlflow-for-mlops-main-components">MLflow for MLOps: main components</h2>
<p><img src="./mlflow-components-mlops.png" alt=""></p>
</section><section>
<h1 id="talk-is-over">Talk is Over</h1>
<br>
<p>Compiled on: 2025-10-28
 — <a href="?print-pdf&amp;pdfSeparateFragments=false"><i class="fa fa-print" aria-hidden="true"></i> printable version</a></p>
<p><a href="#toc"><i class="fa fa-undo" aria-hidden="true"></i> back to ToC</a></p>
</section>

  


</div>
      

    </div>
<script type="text/javascript" src="/talk-2025-mlops/reveal-hugo/object-assign.js"></script>

<a href="/talk-2025-mlops/reveal-js/dist/print/" id="print-location" style="display: none;"></a>

<script type="application/json" id="reveal-hugo-site-params">{"custom_theme":"custom-theme.scss","custom_theme_compile":true,"custom_theme_options":{"enablesourcemap":true,"targetpath":"css/custom-theme.css"},"height":"1080","highlight_theme":"solarized-dark","history":true,"mermaid":[{}],"slide_number":true,"theme":"league","transition":"slide","transition_speed":"fast","width":"1920"}</script>
<script type="application/json" id="reveal-hugo-page-params">null</script>

<script src="/talk-2025-mlops/reveal-js/dist/reveal.js"></script>


  
  
  <script type="text/javascript" src="/talk-2025-mlops/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/talk-2025-mlops/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/talk-2025-mlops/reveal-js/plugin/zoom/zoom.js"></script>
  
  <script type="text/javascript" src="/talk-2025-mlops/reveal-js/plugin/notes/notes.js"></script>
  
  
  <script type="text/javascript" src="/talk-2025-mlops/reveal-js/plugin/notes/notes.js"></script>




<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }

  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };

  var revealHugoPlugins = {
    plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealZoom ]
   };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams),
    camelize(revealHugoPlugins));
  Reveal.initialize(options);
</script>







  
  

  
  

  
  





    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>

<script type="text/javascript" id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>

<script>
  if (/.*?(\?|&)print-pdf/.test(window.location.toString())) {
      var ytVideos = document.getElementsByTagName("iframe")
      for (let i = 0; i < ytVideos.length; i++) {
          var videoFrame = ytVideos[i]
          var isYouTube = /^https?:\/\/(www.)youtube\.com\/.*/.test(videoFrame.src)
          if (isYouTube) {
              console.log(`Removing ${videoFrame.src}`)
              var parent = videoFrame.parentElement
              videoFrame.remove()
              var p = document.createElement('p')
              p.append(
                  document.createTextNode(
                      "There was an embedded video here, but it is disabled in the printed version of the slides."
                  )
              )
              p.append(document.createElement('br'))
              p.append(
                  document.createTextNode(
                      `Visit instead ${
                          videoFrame.src
                      } or ${
                          videoFrame.src.replace(
                              /(^https?:\/\/(www.)youtube\.com)\/(embed\/)(\w+).*/,
                              "https://www.youtube.com/watch?v=$4"
                          )
                      }`
                  )
              )
              parent.appendChild(p)
          }
      }
  }
</script>


    
  

</body></html>